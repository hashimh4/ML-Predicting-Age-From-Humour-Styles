{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: To build and compare predictive models using data. \n",
    "# Define a machine learning problem, including the input and the output and what they are used for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single output (age), many input (humours) --> can we predict a person's age from the combination of humour they tend to use?\n",
    "# Questionnaire results --> Model --> Age\n",
    "# Classification (when age is discrete - use quantile binning) and 2x regression (when age is continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "#import seaborn as sns\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, mean_squared_error, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import copy\n",
    "\n",
    "# Loading a tabular dataset from csv files using pandas\n",
    "data = pd.read_csv(\"HSQ/data.csv\")\n",
    "# We can efficiently analyse all the data we have been given\n",
    "# Our questionnaire acts as a sample for the whole population, so assume that this is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>Q30</th>\n",
       "      <th>Q31</th>\n",
       "      <th>Q32</th>\n",
       "      <th>affiliative</th>\n",
       "      <th>selfenhancing</th>\n",
       "      <th>agressive</th>\n",
       "      <th>selfdefeating</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.025210</td>\n",
       "      <td>3.342670</td>\n",
       "      <td>3.078431</td>\n",
       "      <td>2.833800</td>\n",
       "      <td>3.599440</td>\n",
       "      <td>4.152194</td>\n",
       "      <td>3.277311</td>\n",
       "      <td>2.535014</td>\n",
       "      <td>2.582633</td>\n",
       "      <td>2.869281</td>\n",
       "      <td>...</td>\n",
       "      <td>3.945845</td>\n",
       "      <td>2.767507</td>\n",
       "      <td>2.838469</td>\n",
       "      <td>4.010644</td>\n",
       "      <td>3.375537</td>\n",
       "      <td>2.956583</td>\n",
       "      <td>2.762745</td>\n",
       "      <td>70.966387</td>\n",
       "      <td>1.455649</td>\n",
       "      <td>87.542484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.075782</td>\n",
       "      <td>1.112898</td>\n",
       "      <td>1.167877</td>\n",
       "      <td>1.160252</td>\n",
       "      <td>1.061281</td>\n",
       "      <td>0.979315</td>\n",
       "      <td>1.099974</td>\n",
       "      <td>1.231380</td>\n",
       "      <td>1.224530</td>\n",
       "      <td>1.205013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.135189</td>\n",
       "      <td>1.309601</td>\n",
       "      <td>1.233889</td>\n",
       "      <td>0.708479</td>\n",
       "      <td>0.661533</td>\n",
       "      <td>0.410870</td>\n",
       "      <td>0.645982</td>\n",
       "      <td>1371.989249</td>\n",
       "      <td>0.522076</td>\n",
       "      <td>12.038483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>44849.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Q1           Q2           Q3           Q4           Q5  \\\n",
       "count  1071.000000  1071.000000  1071.000000  1071.000000  1071.000000   \n",
       "mean      2.025210     3.342670     3.078431     2.833800     3.599440   \n",
       "std       1.075782     1.112898     1.167877     1.160252     1.061281   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%       1.000000     3.000000     2.000000     2.000000     3.000000   \n",
       "50%       2.000000     3.000000     3.000000     3.000000     4.000000   \n",
       "75%       3.000000     4.000000     4.000000     4.000000     4.000000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "                Q6           Q7           Q8           Q9          Q10  ...  \\\n",
       "count  1071.000000  1071.000000  1071.000000  1071.000000  1071.000000  ...   \n",
       "mean      4.152194     3.277311     2.535014     2.582633     2.869281  ...   \n",
       "std       0.979315     1.099974     1.231380     1.224530     1.205013  ...   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  ...   \n",
       "25%       4.000000     3.000000     2.000000     2.000000     2.000000  ...   \n",
       "50%       4.000000     3.000000     2.000000     2.000000     3.000000  ...   \n",
       "75%       5.000000     4.000000     3.000000     3.000000     4.000000  ...   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000  ...   \n",
       "\n",
       "               Q30          Q31          Q32  affiliative  selfenhancing  \\\n",
       "count  1071.000000  1071.000000  1071.000000  1071.000000    1071.000000   \n",
       "mean      3.945845     2.767507     2.838469     4.010644       3.375537   \n",
       "std       1.135189     1.309601     1.233889     0.708479       0.661533   \n",
       "min      -1.000000    -1.000000    -1.000000     1.300000       0.000000   \n",
       "25%       3.000000     2.000000     2.000000     3.600000       2.900000   \n",
       "50%       4.000000     3.000000     3.000000     4.100000       3.400000   \n",
       "75%       5.000000     4.000000     4.000000     4.500000       3.800000   \n",
       "max       5.000000     5.000000     5.000000     5.100000       5.000000   \n",
       "\n",
       "         agressive  selfdefeating           age       gender     accuracy  \n",
       "count  1071.000000    1071.000000   1071.000000  1071.000000  1071.000000  \n",
       "mean      2.956583       2.762745     70.966387     1.455649    87.542484  \n",
       "std       0.410870       0.645982   1371.989249     0.522076    12.038483  \n",
       "min       0.000000       0.000000     14.000000     0.000000     2.000000  \n",
       "25%       2.800000       2.300000     18.500000     1.000000    80.000000  \n",
       "50%       3.000000       2.800000     23.000000     1.000000    90.000000  \n",
       "75%       3.300000       3.100000     31.000000     2.000000    95.000000  \n",
       "max       5.000000       5.000000  44849.000000     3.000000   100.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the mean of each column and we can see that our average age is very skewed to the upper age limit\n",
    "# Ensure we have removed invalid rows of data, with incorrect/missing entries (e.g. ages above 123)\n",
    "# Remove any ages above 123 (which is the oldest recorded age rounded up) and 13 (which is the youngest legal age whose data you can store)\n",
    "indexNames = data[(data[\"age\"] > 123) | (data[\"age\"] < 13)].index\n",
    "data = data.drop(indexNames)\n",
    "# Remove any rows with an accuracy of 0 (as these people don't want to be included in research) - not necessary seeing the described data\n",
    "indexNames = data[(data[\"accuracy\"] == 0)].index\n",
    "data = data.drop(indexNames)\n",
    "# Remove any rows where the questions weren't answered with an integer between -1 and 5 (as this is then corrupt data) - not necessary seeing the described data\n",
    "for i in range(1, 32):\n",
    "    indexNames = data[(data[\"Q\" + str(i)] > 5) | (data[\"Q\" + str(i)] < -1)].index\n",
    "    data = data.drop(indexNames)\n",
    "# Remove any rows where gender isn't an integer between 1 and 3\n",
    "indexNames = data[(data[\"gender\"] > 3) | (data[\"gender\"] < 1)].index\n",
    "data = data.drop(indexNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Accuracy\" data spread\n",
    "data[\"accuracy\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Accuracy\" data plot\n",
    "fig, axes = plt.subplots(figsize = (4, 4))\n",
    "data[\"accuracy\"].plot(kind = \"kde\").set_title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want accurate data whilst still having enough\n",
    "# We could add a weighting according to each percentage, but these answer are subjective to the person that gave it\n",
    "# Percentage errors are to do with accuracy. Remove results with an accuracy below 80% which is a comprimise between having enough data and accurate data [find reference]\n",
    "indexNames = data[(data[\"accuracy\"] < 80)].index\n",
    "data = data.drop(indexNames)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the categories (columns) of data we now aren't interested in\n",
    "# Remove all the answers to the questions\n",
    "data = data.drop(data.loc[:, \"Q1\":\"Q32\"].columns, axis = 1)\n",
    "# Remove the gender category as we aren't looking into this\n",
    "data = data.drop([\"gender\"], axis = 1)\n",
    "data = data.drop([\"accuracy\"], axis = 1)\n",
    "\n",
    "# Correct the spelling error of \"agressive\"\n",
    "data.rename(columns = {\"agressive\":\"aggressive\"}, inplace = True)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have defined the input as the questionnaire humour values, and the output as the person's age\n",
    "# Split the data into features x and target y\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data[\"age\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data, but use a constant random seed so that we can compare fairly when testing different models\n",
    "indices = np.arange(data.shape[0])\n",
    "random_gen = np.random.RandomState(4112000) #This won't change since we have set the seed to stay the same\n",
    "permutated = random_gen.permutation(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How our data table looks so far\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot humour styles\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (16, 4))\n",
    "data[\"affiliative\"].plot(kind = \"kde\", ax = axes[0])\n",
    "data[\"selfenhancing\"].plot(kind = \"kde\", ax = axes[0])\n",
    "data[\"aggressive\"].plot(kind = \"kde\", ax = axes[0])\n",
    "data[\"selfdefeating\"].plot(kind = \"kde\", ax = axes[0])\n",
    "axes[0].set_title('Questionnaire Results')\n",
    "\n",
    "# Scale the data to have a mean and standard deviation of zero through standardisation, as we see all our variables follow a bell-shaped curve\n",
    "# We could ideally compare with normalisation and see which works best\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "x_scaled = scaler.transform(x)\n",
    "x = x_scaled\n",
    "\n",
    "# We want to test on the training set, pick the model that then does best on the validation set, and then confirm that this hold true for the test set\n",
    "# Plot age\n",
    "data[\"age\"].plot(kind = \"kde\", ax = axes[1])\n",
    "axes[1].set_title(\"Age\")\n",
    "y_scaled = (y - y.mean()) / y.std()\n",
    "y = y_scaled\n",
    "\n",
    "# Split the data into the training and test sets\n",
    "# Dedicate 70% of the data to training - we have decided on a larger training set\n",
    "# We want to ensure that each set is representative of the whole population\n",
    "# Use a validation set to combat the issue of overfitting\n",
    "training_size = round(0.7 * data.shape[0])\n",
    "validation_size = round((data.shape[0] - training_size)/2)\n",
    "testing_size = data.shape[0] - (training_size + validation_size)\n",
    "\n",
    "# Consisting of affiliative, self-enhancing, agressive and self-defeating scores\n",
    "x_training = x[permutated[:training_size]]\n",
    "x_validation = x[permutated[training_size:(validation_size + training_size)]]\n",
    "x_validation_training = x[permutated[:(training_size + validation_size)]]\n",
    "x_testing = x[permutated[(validation_size + training_size):(validation_size + testing_size + training_size)]]\n",
    "\n",
    "# Consisting of age corresponding to the correct four scores above\n",
    "y_training = y[permutated[:training_size]]\n",
    "y_validation = y[permutated[training_size:(validation_size + training_size)]]\n",
    "y_validation_training = y[permutated[:(training_size + validation_size)]]\n",
    "y_testing = y[permutated[(validation_size + training_size):(validation_size + testing_size + training_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we treat age as ordinal categorical data\n",
    "# Quantile binning age (when used for classification), since age is positively skewed\n",
    "# This will ensure the frequency will be the same in each bracket\n",
    "data_categorical = copy.deepcopy(data)\n",
    "data_categorical[\"age_binned\"], age_bins = pd.qcut(data_categorical[\"age\"], q = 7, retbins = True)\n",
    "data_categorical = data_categorical.drop([\"age\"], axis = 1)\n",
    "print(age_bins)\n",
    "\n",
    "# Print our bins\n",
    "print(data_categorical.head(10))\n",
    "\n",
    "# Transform each of these labels to numbers\n",
    "data_categorical[\"age_binned\"] = LabelEncoder().fit_transform(data_categorical[\"age_binned\"])\n",
    "\n",
    "# This data doesn't follow a bell-shaped curve, so normalise the binned age (instead standardising) to shift the values to center around zero\n",
    "#data_categorical[\"age_binned\"].plot(kind=\"kde\").set_title(\"Binned Age\")\n",
    "#scaler2 = (y_training_c - y_training_c.min()) / (y_training_c.max() - y_training_c.min())\n",
    "#y_scaled_c = (scaler2 * 2) - 1\n",
    "y_c = data_categorical[\"age_binned\"].values\n",
    "scaler2 = (y_c - 3)\n",
    "y_c = scaler2\n",
    "\n",
    "# Redefine the y target values of the training, validation and testing sets, when we are using categorical data\n",
    "# All the corresponding x values will stay the same\n",
    "y_training_c = y_c[permutated[:training_size]]\n",
    "y_validation_c = y_c[permutated[training_size:(validation_size + training_size)]]\n",
    "y_validation_training_c = y_c[permutated[:(training_size + validation_size)]]\n",
    "y_testing_c = y_c[permutated[(validation_size + training_size):(validation_size + testing_size + training_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How our transformed and cleaned data looks so far\n",
    "print(data_categorical.head(5))\n",
    "print(data.head(5))\n",
    "print(data.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inital linear regression model\n",
    "lin_regression = linear_model.LinearRegression()\n",
    "lin_regression.fit(x_training, y_training)\n",
    "# print(lin_regression.coef_)\n",
    "\n",
    "# age_prediction = lin_regression.predict([[3, 2, 1, 4], [-1, 2, 1, 0]])\n",
    "# print(age_prediction)\n",
    "# Predict the validation set\n",
    "lin_regression_predict = lin_regression.predict(x_validation)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_validation, lin_regression_predict))\n",
    "print(\"MSE:\", mean_squared_error(y_validation, lin_regression_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No hyperparameter tuning necessary\n",
    "print(lin_regression.get_params(deep=True).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create this model, fitting the other 80% of the data\n",
    "lin_regression2 = linear_model.LinearRegression()\n",
    "lin_regression.fit(x_validation_training, y_validation_training)\n",
    "\n",
    "# Apply the model to the testing data to predict\n",
    "lin_regression_predict2 = lin_regression.predict(x_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(x_training), len(lin_regression_predict), x_training, lin_regression_predict)\n",
    "# Visualisation for each feature\n",
    "x_testing_col1 = [row[0] for row in x_testing]\n",
    "x_testing_col2 = [row[1] for row in x_testing]\n",
    "x_testing_col3 = [row[2] for row in x_testing]\n",
    "x_testing_col4 = [row[3] for row in x_testing]\n",
    "\n",
    "# We get graphs with a predicted plane, since we don't have 2D data, hence there isn't a single line of best fit\n",
    "plt.suptitle(\"Transformed Age Preditions (Affiliative Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Affiliative Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_testing_col1, y_testing, color = \"green\")\n",
    "plt.scatter(x_testing_col1, lin_regression_predict2, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(\"Transformed Age Preditions (Self-Enhancing Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Self-Enhancing Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_testing_col2, y_testing, color = \"green\")\n",
    "plt.scatter(x_testing_col2, lin_regression_predict2, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(\"Transformed Age Preditions (Aggressive Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Aggressive Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_testing_col3, y_testing, color = \"green\")\n",
    "plt.scatter(x_testing_col3, lin_regression_predict2, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(\"Transformed Age Preditions (Self-Defeating Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Self-Defeating Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_testing_col4, y_testing, color = \"green\")\n",
    "plt.scatter(x_testing_col4, lin_regression_predict2, color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allows us to see how good our current linear model is\n",
    "print(\"MAE:\", mean_absolute_error(y_testing, lin_regression_predict2))\n",
    "print(\"MSE:\", mean_squared_error(y_testing, lin_regression_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inital polynomial regression model\n",
    "degree = PolynomialFeatures(degree = 6)\n",
    "x_poly_training = degree.fit_transform(x_training)\n",
    "poly_regression = linear_model.LinearRegression()\n",
    "poly_regression.fit(x_poly_training, y_training)\n",
    "\n",
    "# Predict the validation set\n",
    "x_poly_validation = degree.fit_transform(x_validation)\n",
    "poly_regression_predict = poly_regression.predict(x_poly_validation)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_validation, poly_regression_predict))\n",
    "print(\"MSE:\", mean_squared_error(y_validation, poly_regression_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with polynomial degree\n",
    "degree.get_params(deep=True).items()\n",
    "\n",
    "# The parameters we want to tune\n",
    "degree2 = [{\"poly__degree\": list(range(2, 3))}]\n",
    "\n",
    "# Create a new empty model and transform the data\n",
    "pipeline = Pipeline(steps=[('poly', PolynomialFeatures()), ('model', linear_model.LinearRegression())])\n",
    "\n",
    "# Use cross-validation (using the validation set separated 4 times (with the fifth section being the test data)) and Grid Search\n",
    "#cv = RepeatedKFold(n_splits = 4, n_repeats = 4, random_state = 1)\n",
    "search = GridSearchCV(pipeline, degree2, scoring = \"neg_root_mean_squared_error\")#, cv = cv)\n",
    "\n",
    "# Fit the best values for the hyperparameters for the model. Use the training data\n",
    "best_poly = search.fit(x_poly_training, y_training)\n",
    "\n",
    "# Print the hyperparameters\n",
    "print(\"Degree:\", best_poly.best_estimator_.get_params()[\"poly__degree\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new best model\n",
    "degree3 = PolynomialFeatures(degree = best_poly.best_estimator_.get_params()[\"poly__degree\"])\n",
    "x_poly_validation_training = degree3.fit_transform(x_validation_training)\n",
    "poly_regression3 = linear_model.LinearRegression()\n",
    "poly_regression3.fit(x_poly_validation_training, y_validation_training)\n",
    "\n",
    "x_poly_testing = degree3.fit_transform(x_testing)\n",
    "poly_regression_predict2 = poly_regression3.predict(x_poly_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print graph similarly to linear regression, to help us visualise our model\n",
    "# Visualise for each feature\n",
    "# We have to refer to different columns when using .fit_transform\n",
    "x_poly_testing_col1 = [row[1] for row in x_poly_testing]\n",
    "x_poly_testing_col2 = [row[2] for row in x_poly_testing]\n",
    "x_poly_testing_col3 = [row[3] for row in x_poly_testing]\n",
    "x_poly_testing_col4 = [row[4] for row in x_poly_testing]\n",
    "#x_poly_testing_col15 = [row[14] for row in x_poly_testing]\n",
    "\n",
    "# We get graphs with a predicted plane, since we don't have 2D data, hence there isn't a single line of best fit\n",
    "plt.suptitle(\"Transformed Age Preditions (Affiliative Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Affiliative Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_poly_testing_col1, y_testing, color = \"green\")\n",
    "plt.scatter(x_poly_testing_col1, poly_regression_predict2, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(\"Transformed Age Preditions (Self-Enhancing Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Self-Enhancing Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_poly_testing_col2, y_testing, color = \"green\")\n",
    "plt.scatter(x_poly_testing_col2, poly_regression_predict2, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(\"Transformed Age Preditions (Aggressive Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Aggressive Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_poly_testing_col3, y_testing, color = \"green\")\n",
    "plt.scatter(x_poly_testing_col3, poly_regression_predict2, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(\"Transformed Age Preditions (Self-Defeating Humour)\", fontsize = 14)\n",
    "plt.xlabel(\"Self-Defeating Humour\", fontsize = 10)\n",
    "plt.ylabel(\"Age\", fontsize = 12)\n",
    "plt.scatter(x_poly_testing_col4, y_testing, color = \"green\")\n",
    "plt.scatter(x_poly_testing_col4, poly_regression_predict2, color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to see how good our current polynomial model is\n",
    "print(\"MAE:\", mean_absolute_error(y_testing, poly_regression_predict2))\n",
    "print(\"MSE:\", mean_squared_error(y_testing, poly_regression_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 6, leaf_size = 20, p = 1)\n",
    "knn.fit(x_training, y_training_c)\n",
    "\n",
    "# Predict validation set\n",
    "knn_predict = knn.predict(x_validation)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_validation_c, knn_predict))\n",
    "print(\"Accuracy score:\", accuracy_score(y_testing_c, knn_predict))\n",
    "print(\"Classification report:\", classification_report(y_validation_c, knn_predict))\n",
    "# Each line of our classification report refers to each bin we previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with leaf size (the maximum number of points belonging to each leaf node) and the number of neigbors (classifications)\n",
    "print(knn.get_params(deep=True).items())\n",
    "\n",
    "# Create a new empty model\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "# The parameters we want to tune\n",
    "leaf_size = list(range(1, 50))\n",
    "neighbors = list(range(1, 30))\n",
    "p = [1, 2]\n",
    "metric = [\"minkowski\", \"hamming\", \"manhattan\", \"euclidean\"]\n",
    "\n",
    "# Converting the parameters to a dictionary\n",
    "hyperparameters = dict(leaf_size = leaf_size, n_neighbors = neighbors, p = p, metric = metric)\n",
    "\n",
    "# Use cross-validation (using the validation set separated 4 times (with the fifth section being the test data)) and Grid Search\n",
    "cv = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 4, random_state = 1)\n",
    "search = GridSearchCV(knn2, hyperparameters, scoring = \"accuracy\", cv = cv)\n",
    "\n",
    "# Fit the best values for the hyperparameters for the model. Use the training data\n",
    "best_knn = search.fit(x_training, y_training_c)\n",
    "\n",
    "# Print the hyperparameters\n",
    "print(\"Best leaf_size:\", best_knn.best_estimator_.get_params()['leaf_size'])\n",
    "print(\"Best p:\", best_knn.best_estimator_.get_params()['p'])\n",
    "print(\"Best n_neighbors:\", best_knn.best_estimator_.get_params()['n_neighbors'])\n",
    "print(\"Best metric:\", best_knn.best_estimator_.get_params()[\"metric\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new best model\n",
    "knn3 = KNeighborsClassifier(n_neighbors = best_knn.best_estimator_.get_params()[\"n_neighbors\"], leaf_size = best_knn.best_estimator_.get_params()[\"leaf_size\"], p = best_knn.best_estimator_.get_params()[\"p\"], metric = best_knn.best_estimator_.get_params()[\"metric\"])\n",
    "knn3.fit(x_validation_training, y_validation_training_c)\n",
    "\n",
    "knn_predict2 = knn3.predict(x_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to see how good our current knn model is\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_testing_c, knn_predict2))\n",
    "print(\"Accuracy score:\", accuracy_score(y_testing_c, knn_predict2))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_testing_c, knn_predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For polynomial regression, the cross-validation code is commented out, since it was taking long to run on my device\n",
    "# I also changed the hyperparameter polynomial degree from range(2, 4) to range(2, 3) since it was overloading my device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
